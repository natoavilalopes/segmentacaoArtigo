{"cells":[{"cell_type":"markdown","metadata":{"id":"OXCJGXe5PCBF"},"source":["Instalar a biblioteca `ultralytics`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":82024,"status":"ok","timestamp":1724265879279,"user":{"displayName":"Segmentacao Artigo","userId":"15536144794875926342"},"user_tz":180},"id":"rZBS_yayEtbV","outputId":"5543a0e7-6e91-4581-d289-30dd7c13fa72"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting ultralytics\n","  Downloading ultralytics-8.2.79-py3-none-any.whl.metadata (41 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.25.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.1+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.1+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.4)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.5-py3-none-any.whl.metadata (8.9 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.1)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n","  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n","Downloading ultralytics-8.2.79-py3-none-any.whl (869 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.1/869.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Downloading ultralytics_thop-2.0.5-py3-none-any.whl (25 kB)\n","Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 ultralytics-8.2.79 ultralytics-thop-2.0.5\n"]}],"source":["!pip install ultralytics"]},{"cell_type":"markdown","metadata":{"id":"oAV-MgElnj7y"},"source":["Criar diretório `segmentacao` e adicionar as pastas `model` e `videos` copiadas do Google Drive.\n","\n","O arquivo `best.pt` da pasta `model` foi gerado pelo treinamento da rede utilizando este outro Colab:\n","\n","https://colab.research.google.com/drive/1fyGCk_hMf-kYg62ASMaOle7XChxBypXY\n","\n","Já o arquivo `data.yaml`, nessa mesma pasta, foi gerado utilizando o Roboflow, como explicando no Colab desse link.\n","\n","Os vídeos da pasta `videos` não foram utilizadas nesse treinamento e serão úteis para testar o modelo treinado."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":23656,"status":"ok","timestamp":1724270350333,"user":{"displayName":"Segmentacao Artigo","userId":"15536144794875926342"},"user_tz":180},"id":"I97lUMQJGsOG","outputId":"fc8e9d1d-d929-4df7-a572-1a4617f5b077"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?export=download&id=15kE3MNhMV0_xXv6rlPpwNTBELnN3s0Vg\n","To: /content/best.pt\n","100%|██████████| 54.8M/54.8M [00:02<00:00, 22.3MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?export=download&id=1BAy8QVuHlUByOWzx_bVOTEopRIssU-Lr\n","To: /content/data.yaml\n","100%|██████████| 334/334 [00:00<00:00, 1.37MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?export=download&id=1_ig46XMu-IgwEdfPmL8zjK35X3CW9eFb\n","To: /content/estrada01.mp4\n","100%|██████████| 12.1M/12.1M [00:00<00:00, 29.2MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?export=download&id=1xEG57VWTMY9AWFAAx2WZZgqsPl42BA7t\n","To: /content/estrada05.mp4\n","100%|██████████| 3.04M/3.04M [00:00<00:00, 20.7MB/s]\n"]}],"source":["!mkdir /content/segmentacao\n","!mkdir /content/segmentacao/model/\n","!mkdir /content/segmentacao/videos/\n","!mkdir /content/segmentacao/out/\n","\n","import gdown\n","\n","#https://drive.google.com/file/d/15kE3MNhMV0_xXv6rlPpwNTBELnN3s0Vg/view?usp=sharing\n","file_id = '15kE3MNhMV0_xXv6rlPpwNTBELnN3s0Vg'\n","gdown.download(f'https://drive.google.com/uc?export=download&id={file_id}', 'best.pt', quiet=False)\n","!mv best.pt /content/segmentacao/model/\n","\n","# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n","\n","#https://drive.google.com/file/d/1BAy8QVuHlUByOWzx_bVOTEopRIssU-Lr/view?usp=sharing\n","file_id = '1BAy8QVuHlUByOWzx_bVOTEopRIssU-Lr'\n","gdown.download(f'https://drive.google.com/uc?export=download&id={file_id}', 'data.yaml', quiet=False)\n","!mv data.yaml /content/segmentacao/model/\n","\n","# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n","\n","#https://drive.google.com/file/d/1_ig46XMu-IgwEdfPmL8zjK35X3CW9eFb/view?usp=sharing\n","file_id = '1_ig46XMu-IgwEdfPmL8zjK35X3CW9eFb'\n","gdown.download(f'https://drive.google.com/uc?export=download&id={file_id}', 'estrada01.mp4', quiet=False)\n","!mv estrada01.mp4 /content/segmentacao/videos/\n","\n","\n","#https://drive.google.com/file/d/1xEG57VWTMY9AWFAAx2WZZgqsPl42BA7t/view?usp=sharing\n","file_id = '1xEG57VWTMY9AWFAAx2WZZgqsPl42BA7t'\n","gdown.download(f'https://drive.google.com/uc?export=download&id={file_id}', 'estrada05.mp4', quiet=False)\n","!mv estrada05.mp4 /content/segmentacao/videos/"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"executionInfo":{"elapsed":8,"status":"error","timestamp":1724270306704,"user":{"displayName":"Segmentacao Artigo","userId":"15536144794875926342"},"user_tz":180},"id":"tYSWwfgpExBa","colab":{"base_uri":"https://localhost:8080/","height":298},"outputId":"988f6e12-3b97-4aec-c71a-2eac4fb621a0"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/segmentacao/model/data.yaml'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-5556169389e4>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mnomeClasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-1-5556169389e4>\u001b[0m in \u001b[0;36mload_classes\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#with open('/data.yaml', 'r') as config_file:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/segmentacao/model/data.yaml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconfig_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mconfig_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/segmentacao/model/data.yaml'"]}],"source":["def load_classes():\n","    #with open('/data.yaml', 'r') as config_file:\n","    with open('/content/segmentacao/model/data.yaml', 'r') as config_file:\n","        config_data = config_file.read()\n","\n","    class_names_start = config_data.find(\"names: [\") + len(\"names: [\")\n","    class_names_end = config_data.find(\"]\", class_names_start)\n","    class_names_str = config_data[class_names_start:class_names_end]\n","    class_names = [name.strip().strip(\"'\") for name in class_names_str.split(\",\")]\n","\n","    return class_names\n","\n","nomeClasses = load_classes()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vrddmDE7GcBR"},"outputs":[],"source":["import random\n","\n","def class_colors(names):\n","    return {name: (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)) for name in names}\n","\n","color = class_colors(nomeClasses)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Y-ps_zvh33I"},"outputs":[],"source":["import cv2\n","\n","dev = '/content/segmentacao/videos/estrada05.mp4'\n","cap = cv2.VideoCapture(dev)\n","\n","frame_width = int(cap.get(3))\n","frame_height = int(cap.get(4))\n","\n","size = (frame_width, frame_height)\n","\n","#result = cv2.VideoWriter(path + \"file00.mp4\", cv2.VideoWriter_fourcc(*'MJPG'), 5, size)\n","#result = cv2.VideoWriter(path + \"output.mp4\", -1, 5, size)\n","\n","fourcc = cv2.VideoWriter_fourcc(*'XVID')\n","result = cv2.VideoWriter('/content/segmentacao/out/output_video.avi', fourcc, 5, size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OtFTN7TyUvw2"},"outputs":[],"source":["from ultralytics import YOLO\n","import numpy as np\n","\n","# Carregar o modelo treinado YOLOv8\n","#model = YOLO('/content/segmentacao/model/best.pt')\n","\n","class YOLOSegmentation:\n","    def __init__(self, model_path):\n","        self.model = YOLO(model_path)\n","\n","    def detect(self, img):\n","        # Get img shape\n","\n","        height, width, channels = img.shape\n","\n","        results = self.model.predict(source=img.copy(), save=False, save_txt=False)\n","        result = results[0]\n","        segmentation_contours_idx = []\n","        #for seg in result.masks.segments:\n","        for seg in result.masks.xyn:\n","            # contours\n","            seg[:, 0] *= width\n","            seg[:, 1] *= height\n","            segment = np.array(seg, dtype=np.int32)\n","            segmentation_contours_idx.append(segment)\n","\n","        bboxes = np.array(result.boxes.xyxy.cpu(), dtype=\"int\")\n","        # Get class ids\n","        class_ids = np.array(result.boxes.cls.cpu(), dtype=\"int\")\n","        # Get scores\n","        scores = np.array(result.boxes.conf.cpu(), dtype=\"float\").round(2)\n","        return bboxes, class_ids, segmentation_contours_idx, scores\n","\n","\n","model = YOLOSegmentation('/content/segmentacao/model/best.pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":135407,"status":"ok","timestamp":1724266357783,"user":{"displayName":"Segmentacao Artigo","userId":"15536144794875926342"},"user_tz":180},"id":"dfXtJjyGiSMo","outputId":"515cacad-09fc-4876-8702-d23831fee896"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 1747.1ms\n","Speed: 16.5ms preprocess, 1747.1ms inference, 56.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 1534.6ms\n","Speed: 3.8ms preprocess, 1534.6ms inference, 15.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 1542.7ms\n","Speed: 3.5ms preprocess, 1542.7ms inference, 18.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 2357.9ms\n","Speed: 3.4ms preprocess, 2357.9ms inference, 23.9ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 1996.2ms\n","Speed: 5.0ms preprocess, 1996.2ms inference, 18.0ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 1563.8ms\n","Speed: 3.2ms preprocess, 1563.8ms inference, 14.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 1520.3ms\n","Speed: 3.1ms preprocess, 1520.3ms inference, 17.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 1579.3ms\n","Speed: 3.5ms preprocess, 1579.3ms inference, 14.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 1526.4ms\n","Speed: 3.2ms preprocess, 1526.4ms inference, 14.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 1516.8ms\n","Speed: 3.9ms preprocess, 1516.8ms inference, 14.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 1832.1ms\n","Speed: 3.3ms preprocess, 1832.1ms inference, 27.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 2460.7ms\n","Speed: 6.6ms preprocess, 2460.7ms inference, 27.9ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 1604.6ms\n","Speed: 3.6ms preprocess, 1604.6ms inference, 14.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 1543.9ms\n","Speed: 3.4ms preprocess, 1543.9ms inference, 18.9ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1544.2ms\n","Speed: 5.2ms preprocess, 1544.2ms inference, 29.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1555.6ms\n","Speed: 3.6ms preprocess, 1555.6ms inference, 21.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1526.9ms\n","Speed: 3.1ms preprocess, 1526.9ms inference, 20.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1586.9ms\n","Speed: 3.4ms preprocess, 1586.9ms inference, 20.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 2345.2ms\n","Speed: 3.1ms preprocess, 2345.2ms inference, 29.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1992.7ms\n","Speed: 7.8ms preprocess, 1992.7ms inference, 21.0ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1551.8ms\n","Speed: 3.1ms preprocess, 1551.8ms inference, 17.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1529.8ms\n","Speed: 3.6ms preprocess, 1529.8ms inference, 21.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1529.5ms\n","Speed: 3.1ms preprocess, 1529.5ms inference, 17.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 1537.3ms\n","Speed: 3.9ms preprocess, 1537.3ms inference, 15.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1789.8ms\n","Speed: 3.1ms preprocess, 1789.8ms inference, 20.9ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 3009.4ms\n","Speed: 3.1ms preprocess, 3009.4ms inference, 27.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 4 skys, 2 vegetations, 1921.0ms\n","Speed: 3.3ms preprocess, 1921.0ms inference, 24.0ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 4 skys, 2 vegetations, 1549.8ms\n","Speed: 3.4ms preprocess, 1549.8ms inference, 19.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1507.7ms\n","Speed: 3.1ms preprocess, 1507.7ms inference, 21.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 4 skys, 2 vegetations, 1552.9ms\n","Speed: 3.2ms preprocess, 1552.9ms inference, 24.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1537.3ms\n","Speed: 3.2ms preprocess, 1537.3ms inference, 20.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 4 skys, 2 vegetations, 1576.7ms\n","Speed: 3.2ms preprocess, 1576.7ms inference, 19.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1929.3ms\n","Speed: 3.3ms preprocess, 1929.3ms inference, 36.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 2441.9ms\n","Speed: 3.1ms preprocess, 2441.9ms inference, 17.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1534.1ms\n","Speed: 6.0ms preprocess, 1534.1ms inference, 20.9ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 4 skys, 2 vegetations, 1562.9ms\n","Speed: 3.1ms preprocess, 1562.9ms inference, 19.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1549.2ms\n","Speed: 7.4ms preprocess, 1549.2ms inference, 22.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 4 skys, 2 vegetations, 1560.8ms\n","Speed: 4.2ms preprocess, 1560.8ms inference, 19.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1557.6ms\n","Speed: 3.0ms preprocess, 1557.6ms inference, 20.9ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1665.6ms\n","Speed: 3.1ms preprocess, 1665.6ms inference, 36.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 4 skys, 2 vegetations, 2509.4ms\n","Speed: 3.2ms preprocess, 2509.4ms inference, 37.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1639.8ms\n","Speed: 3.1ms preprocess, 1639.8ms inference, 16.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1490.1ms\n","Speed: 3.2ms preprocess, 1490.1ms inference, 17.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1570.4ms\n","Speed: 3.5ms preprocess, 1570.4ms inference, 22.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 1507.1ms\n","Speed: 3.9ms preprocess, 1507.1ms inference, 14.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 1512.0ms\n","Speed: 4.9ms preprocess, 1512.0ms inference, 14.2ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 4 vegetations, 1529.2ms\n","Speed: 3.2ms preprocess, 1529.2ms inference, 23.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 2116.3ms\n","Speed: 3.5ms preprocess, 2116.3ms inference, 27.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 2109.9ms\n","Speed: 3.3ms preprocess, 2109.9ms inference, 14.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 1537.9ms\n","Speed: 3.3ms preprocess, 1537.9ms inference, 17.9ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 1573.9ms\n","Speed: 3.7ms preprocess, 1573.9ms inference, 14.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1536.4ms\n","Speed: 3.2ms preprocess, 1536.4ms inference, 16.7ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1555.0ms\n","Speed: 5.6ms preprocess, 1555.0ms inference, 17.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1562.2ms\n","Speed: 3.6ms preprocess, 1562.2ms inference, 19.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1809.4ms\n","Speed: 5.8ms preprocess, 1809.4ms inference, 32.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 2494.0ms\n","Speed: 3.1ms preprocess, 2494.0ms inference, 33.3ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1568.7ms\n","Speed: 5.2ms preprocess, 1568.7ms inference, 17.0ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 4 skys, 2 vegetations, 1548.9ms\n","Speed: 3.2ms preprocess, 1548.9ms inference, 19.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1527.0ms\n","Speed: 4.4ms preprocess, 1527.0ms inference, 22.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1556.8ms\n","Speed: 3.1ms preprocess, 1556.8ms inference, 22.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 3 skys, 2 vegetations, 1568.8ms\n","Speed: 3.2ms preprocess, 1568.8ms inference, 17.4ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 1535.5ms\n","Speed: 3.2ms preprocess, 1535.5ms inference, 17.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 3082.3ms\n","Speed: 3.2ms preprocess, 3082.3ms inference, 60.1ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 rural_roads, 2 skys, 2 vegetations, 3849.4ms\n","Speed: 6.0ms preprocess, 3849.4ms inference, 25.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 rural_roads, 2 skys, 2 vegetations, 1569.4ms\n","Speed: 3.6ms preprocess, 1569.4ms inference, 20.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 1660.4ms\n","Speed: 3.2ms preprocess, 1660.4ms inference, 14.5ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 1583.6ms\n","Speed: 5.8ms preprocess, 1583.6ms inference, 23.8ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 1 rural_road, 2 skys, 2 vegetations, 1572.9ms\n","Speed: 4.0ms preprocess, 1572.9ms inference, 14.6ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 rural_roads, 2 skys, 2 vegetations, 1547.9ms\n","Speed: 3.2ms preprocess, 1547.9ms inference, 21.0ms postprocess per image at shape (1, 3, 480, 640)\n","\n","0: 480x640 2 rural_roads, 3 skys, 2 vegetations, 2325.3ms\n","Speed: 5.6ms preprocess, 2325.3ms inference, 32.3ms postprocess per image at shape (1, 3, 480, 640)\n"]}],"source":["while True:\n","    ret, frame = cap.read()\n","    if not ret:\n","      break\n","\n","    bboxes, classes, segmentations, scores = model.detect(frame)\n","\n","    for bbox, class_id, seg, score in zip(bboxes, classes, segmentations, scores):\n","      (x, y, x2, y2) = bbox\n","      className = nomeClasses[class_id]\n","\n","      cv2.polylines(frame, [seg], True, color[className], 3)\n","\n","      result.write(frame)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PUheI05bhfTf"},"outputs":[],"source":["from IPython.display import HTML\n","from base64 import b64encode\n","\n","def show_video(video_path, video_width = 640):\n","\n","  video_file = open(video_path, \"r+b\").read()\n","\n","  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n","  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":501,"output_embedded_package_id":"1zj36AwscbgMnpPFNPRiNFwN6GaDa93UW"},"executionInfo":{"elapsed":16253,"status":"ok","timestamp":1724266374014,"user":{"displayName":"Segmentacao Artigo","userId":"15536144794875926342"},"user_tz":180},"id":"wX9i9943Zv9N","outputId":"5fe957fd-a1cd-4c95-9f2c-d9b44c1f5ea1"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["show_video('/content/segmentacao/videos/estrada05.mp4')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"output_embedded_package_id":"1EZZ0dx2mMhCCDnB9qwknz7cs61s1USwV"},"id":"zXkuyKBPBpYO","outputId":"5e3bf4f6-5a56-4e4d-93d2-d83d479cf9d0"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["show_video('/content/segmentacao/out/output_video.avi')"]},{"cell_type":"markdown","metadata":{"id":"8H06jdFTleJV"},"source":["Se o video não funcionar, faça o download para visualizá-lo."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jR0XcPmwBpez","outputId":"82719648-da47-48ee-92de-b99334ecf2fc"},"outputs":[{"data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/javascript":["download(\"download_8fc430aa-7ac3-4f7d-b27f-7ebdc6c85610\", \"output_video.avi\", 14864520)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"}],"source":["from google.colab import files\n","files.download('/content/segmentacao/out/output_video.avi')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dkyorB8UBpi3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B2I8pkw1mcHo"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OgjFkJDtBpnE"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"durbIck3Bpq1"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"hUCXG8hHYG2m"},"source":["Até aqui\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X_p2DEfOBpum"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ceIafgyzBpzA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yO22_BR0Bp5n"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-xwsDu-wBp9e"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pUL3ceGJFvGU"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UG_pmTniFvJe"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"92bBT-y_FvMU"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wet1KrJNFvPm"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"56IaJQ1bFvTG"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pzyer9ksFvWi"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XtpIrcraFvaS"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9qD_Hc7HFvdh"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f8sFCWRvFvhL"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pr9yLDKcFvk7"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-stk_m-h3rL"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h_EpQ-qYh3uF"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rj6oW3FWh3xy"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"josUYzNcZwAZ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dXOlrU8bZwDb"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J5v3fr1aZwGp"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aLVXiIJIZwI_"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qSN_SDPVZwO2"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OFtnH_6hXJ8D"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k8_eIU2GXKBq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OxaHu5w_XKE_"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nv_5TlDxXKIQ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xGjIrtEFXKLi"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AyD8RMg9XKPL"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FnK-NpAhXKTA"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ny8J8WUtXKWO"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p0r8ndNKXKZy"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1V8z_jbMoqU1OoTbn9Ikqfs_7CcFVp24o","timestamp":1723657146867},{"file_id":"1iTT9vSDx-0Rz1fHYPOXlDZuHmH5KAajr","timestamp":1723041567379}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}